{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a35eeb9f-df70-4ab1-a243-2d2025888eb0",
      "cell_type": "markdown",
      "source": "# Introduction to the JupyterLab and Jupyter Notebooks\n\nThis is a short introduction to two of the flagship tools created by [the Jupyter Community](https://jupyter.org).\n\n> **âš ï¸Experimental!âš ï¸**: This is an experimental interface provided by the [JupyterLite project](https://jupyterlite.readthedocs.io/en/latest/). It embeds an entire JupyterLab interface, with many popular packages for scientific computing, in your browser. There may be minor differences in behavior between JupyterLite and the JupyterLab you install locally. You may also encounter some bugs or unexpected behavior. To report any issues, or to get involved with the JupyterLite project, see [the JupyterLite repository](https://github.com/jupyterlite/jupyterlite/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc).\n\n## JupyterLab ðŸ§ª\n\n**JupyterLab** is a next-generation web-based user interface for Project Jupyter. It enables you to work with documents and activities such as Jupyter notebooks, text editors, terminals, and custom components in a flexible, integrated, and extensible manner. It is the interface that you're looking at right now.\n\n**For an overview of the JupyterLab interface**, see the **JupyterLab Welcome Tour** on this page, by going to `Help -> Welcome Tour` and following the prompts.\n\n> **See Also**: For a more in-depth tour of JupyterLab with a full environment that runs in the cloud, see [the JupyterLab introduction on Binder](https://mybinder.org/v2/gh/jupyterlab/jupyterlab-demo/HEAD?urlpath=lab/tree/demo).\n\n## Jupyter Notebooks ðŸ““\n\n**Jupyter Notebooks** are a community standard for communicating and performing interactive computing. They are a document that blends computations, outputs, explanatory text, mathematics, images, and rich media representations of objects.\n\nJupyterLab is one interface used to create and interact with Jupyter Notebooks.\n\n**For an overview of Jupyter Notebooks**, see the **JupyterLab Welcome Tour** on this page, by going to `Help -> Notebook Tour` and following the prompts.\n\n> **See Also**: For a more in-depth tour of Jupyter Notebooks and the Classic Jupyter Notebook interface, see [the Jupyter Notebook IPython tutorial on Binder](https://mybinder.org/v2/gh/ipython/ipython-in-depth/HEAD?urlpath=tree/binder/Index.ipynb).\n\n## An example: visualizing data in the notebook âœ¨\n\nBelow is an example of a code cell. We'll visualize some simple data using two popular packages in Python. We'll use [NumPy](https://numpy.org/) to create some random data, and [Matplotlib](https://matplotlib.org) to visualize it.\n\nNote how the code and the results of running the code are bundled together.",
      "metadata": {}
    },
    {
      "id": "69e884b3-6a41-4790-b040-db18a366e216",
      "cell_type": "code",
      "source": "Objective:\nPredict housing prices using both structured (tabular) data and real house images.\n\nApproach:\n1. Load a public dataset of US houses (metadata + images).\n2. Preprocess and combine tabular features (bedrooms, bathrooms, area, zipcode) with CNN-extracted image features.\n3. Train a multimodal regression model.\n4. Evaluate using MAE & RMSE.\n5. Provide visualizations and insights.\n\nSkills Gained:\n- Multimodal Machine Learning\n- Convolutional Neural Networks (CNNs)\n- Feature fusion (image + tabular)\n- Regression modeling and evaluation",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5acb57b1-5ec1-44ef-b840-2c8aa754fb74",
      "cell_type": "code",
      "source": "# =========================================================\n# 2. Dataset Loading & Preprocessing\n# =========================================================\n!git clone -q https://github.com/emanhamed/Houses-dataset.git\n\nimport os, glob, math, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras import layers, models\n\n# Paths\nDATA_DIR = \"/content/Houses-dataset/Houses Dataset\"\nmeta_path = os.path.join(DATA_DIR, \"HousesInfo.txt\")\n\n# Load tabular data\ncols = [\"bed\", \"bath\", \"area\", \"zipcode\", \"price\"]\ndf = pd.read_csv(meta_path, delim_whitespace=True, header=None, names=cols)\n\n# Image loader: create 2x2 montage per house\ndef load_house_montage(idx, base_path=DATA_DIR, tile=128):\n    paths = sorted(glob.glob(os.path.join(base_path, f\"{idx+1}_*\")))\n    if len(paths) < 4:\n        return None\n    imgs = []\n    for p in paths[:4]:\n        img = cv2.imread(p)\n        if img is None:\n            return None\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (tile, tile))\n        imgs.append(img)\n    top = np.hstack((imgs[0], imgs[1]))\n    bottom = np.hstack((imgs[2], imgs[3]))\n    return np.vstack((top, bottom))\n\n# Build image dataset\nimages, keep_indices = [], []\nfor i in range(len(df)):\n    m = load_house_montage(i)\n    if m is not None:\n        images.append(m)\n        keep_indices.append(i)\ndf = df.iloc[keep_indices].reset_index(drop=True)\nimages = np.array(images, dtype=\"uint8\")\n\n# Preview\nplt.figure(figsize=(12,4))\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    plt.imshow(images[i])\n    plt.title(f\"${df['price'][i]:,.0f} | {df['bed'][i]}bd/{df['bath'][i]}ba\")\n    plt.axis('off')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3543ebef-43a6-4db2-9099-58c6dffb8a57",
      "cell_type": "code",
      "source": "# =========================================================\n# 3. Model Development & Training\n# =========================================================\n# Split\nX_tab = df[[\"bed\", \"bath\", \"area\", \"zipcode\"]]\ny = df[\"price\"].astype(float)\nX_img = images.copy()\n\nX_tab_train, X_tab_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n    X_tab, X_img, y, test_size=0.2, random_state=42\n)\n\n# Preprocess tabular\nnumeric_features = [\"bed\", \"bath\", \"area\"]\ncategorical_features = [\"zipcode\"]\ntab_preproc = ColumnTransformer([\n    (\"num\", StandardScaler(), numeric_features),\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n])\nX_tab_train_proc = tab_preproc.fit_transform(X_tab_train)\nX_tab_test_proc = tab_preproc.transform(X_tab_test)\n\n# Image feature extractor\nIMG_INPUT = 224\ndef prep_images(x):\n    x = tf.image.resize(x, (IMG_INPUT, IMG_INPUT))\n    return preprocess_input(x)\n\nbase_cnn = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(IMG_INPUT, IMG_INPUT, 3))\nbase_cnn.trainable = False\n\nimg_input = tf.keras.Input(shape=(X_img_train.shape[1], X_img_train.shape[2], 3))\nx = layers.Lambda(prep_images)(img_input)\nx = base_cnn(x)\nx = layers.GlobalAveragePooling2D()(x)\nimg_encoder = tf.keras.Model(img_input, x, name=\"image_encoder\")\n\n# Extract embeddings\nimg_train_embed = img_encoder.predict(X_img_train, verbose=0)\nimg_test_embed = img_encoder.predict(X_img_test, verbose=0)\n\n# Fusion model\nin_img = tf.keras.Input(shape=(img_train_embed.shape[1],))\nin_tab = tf.keras.Input(shape=(X_tab_train_proc.shape[1],))\nz = layers.Concatenate()([in_img, in_tab])\nz = layers.Dense(256, activation=\"relu\")(z)\nz = layers.Dropout(0.3)(z)\nz = layers.Dense(128, activation=\"relu\")(z)\nz = layers.Dropout(0.2)(z)\nout = layers.Dense(1, activation=\"linear\")(z)\nfusion_model = tf.keras.Model([in_img, in_tab], out)\n\nfusion_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])\nhistory = fusion_model.fit(\n    [img_train_embed, X_tab_train_proc], y_train,\n    validation_split=0.2,\n    epochs=50, batch_size=16, verbose=1\n)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9e709922-8e9d-46bd-bfb0-48d1eee810fa",
      "cell_type": "code",
      "source": "# =========================================================\n# 4. Evaluation\n# =========================================================\ny_pred = fusion_model.predict([img_test_embed, X_tab_test_proc]).ravel()\nmae = mean_absolute_error(y_test, y_pred)\nrmse = math.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"MAE: ${mae:,.0f}\")\nprint(f\"RMSE: ${rmse:,.0f}\")\n\n# Plot Actual vs Predicted\nplt.figure(figsize=(6,6))\nplt.scatter(y_test, y_pred, alpha=0.7)\nmn, mx = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\nplt.plot([mn, mx], [mn, mx], 'r--')\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Actual vs Predicted Prices\")\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "12845166-01e8-4cb2-8b9b-2bac9064e461",
      "cell_type": "code",
      "source": "# =========================================================\n# 5. Final Summary / Insights\n# =========================================================\n\"\"\"\nSummary:\n- Successfully combined CNN image features with preprocessed tabular data.\n- Achieved competitive MAE and RMSE on the test set.\n- Visual analysis shows predictions closely follow actual prices.\n- This approach demonstrates the power of multimodal learning in real estate price prediction.\n\nThanks to DeveloperHub Corporation for the opportunity and guidance, and special thanks to the mentors and team members involved.\n\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b9670d85-ef29-4dfd-b4b5-e76d279c1f1a",
      "cell_type": "markdown",
      "source": "## Next steps ðŸƒ\n\nThis is just a short introduction to JupyterLab and Jupyter Notebooks. See below for some more ways to interact with tools in the Jupyter ecosystem, and its community.\n\n### Other notebooks in this demo\n\nHere are some other notebooks in this demo. Each of the items below corresponds to a file or folder in the **file browser to the left**.\n\n- [**`Lorenz.ipynb`**](Lorenz.ipynb) uses Python to demonstrate interactive visualizations and computations around the [Lorenz system](https://en.wikipedia.org/wiki/Lorenz_system). It shows off basic Python functionality, including more visualizations, data structures, and scientific computing libraries.\n- [**`r.ipynb`**](r.ipynb) demonstrates the R programming language for statistical computing and data analysis.\n- [**`cpp.ipynb`**](cpp.ipynb) demonstrates the C++ programming language for scientific computing and data analysis.\n- [**`sqlite.ipynb`**](sqlite.ipynb) demonstrates how an in-browser sqlite kernel to run your own SQL commands from the notebook. It uses the [jupyterlite/xeus-sqlite-kernel](https://github.com/jupyterlite/xeus-sqlite-kernel).\n\n### Other sources of information in Jupyter\n\n- **More on using JupyterLab**: See [the JupyterLab documentation](https://jupyterlab.readthedocs.io/en/stable/) for more thorough information about how to install and use JupyterLab.\n- **More interactive demos**: See [try.jupyter.org](https://try.jupyter.org) for more interactive demos with the Jupyter ecosystem.\n- **Learn more about Jupyter**: See [the Jupyter community documentation](https://docs.jupyter.org) to learn more about the project, its community and tools, and how to get involved.\n- **Join our discussions**: The [Jupyter Community Forum](https://discourse.jupyter.org) is a place where many in the Jupyter community ask questions, help one another, and discuss issues around interactive computing and our ecosystem.",
      "metadata": {}
    }
  ]
}